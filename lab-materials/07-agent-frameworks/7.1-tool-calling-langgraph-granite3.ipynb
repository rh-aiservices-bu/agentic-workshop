{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5f1d39-2310-42ae-9477-4cde497374cc",
   "metadata": {},
   "source": [
    "## Tool Calling with Agentic AI - LangGraph\n",
    "\n",
    "### LLM Used - Granite3.1-8B\n",
    "\n",
    "In this notebook we will learn how to use Tool Calling with Agentic AI in order to solve different problems.\n",
    "\n",
    "Tool-calling agents expand the capabilities of an LLM by allowing it to interact with external systems. This approach empowers agents to dynamically solve problems by utilizing tools, accessing memory, and planning multi-step actions.\n",
    "\n",
    "Tool calling agents enable:\n",
    "\n",
    "1. Multi-Step Decision Making: The LLM can orchestrate a sequence of decisions to achieve complex objectives.\n",
    "2. Tool Access: The LLM can select and use various tools as needed to interact with external systems and APIs.\n",
    "\n",
    "This architecture allows for more dynamic and flexible behaviors, enabling agents to solve complex tasks by leveraging external resources efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "Import necessary libraries, including `langchain_core`, `langgraph`, and `langchain_community`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03127548-960c-4565-9648-6137eea0010b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langgraph==0.2.35 langchain_experimental==0.0.65 langchain-openai==0.1.25 termcolor==2.3.0 duckduckgo_search==7.1.0 openapi-python-client==0.12.3 langchain_community==0.2.19 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf493b9-622f-49dc-a81b-768dabc5139e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agentic AI libraries\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "### Tool for checking externally the app\n",
    "\n",
    "Define the tools that the agent will use, such as a simulated web search for weather and an analysis tool for technical questions.\n",
    "\n",
    "First you need to deploy the middleware app in OpenShift that will give us access to the Weather Tool Retriever API.\n",
    "\n",
    "Deploy the weather application with the following:\n",
    "\n",
    "```bash\n",
    "kubectl apply -k agents/weather-app/gitops \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5de15c-6645-4d77-b06d-936aa9487c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search tool\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Get weather information from weather-app middleware.\"\"\"\n",
    "\n",
    "    # Extract city name using Regex\n",
    "    match = re.search(r\"\\b(?:weather in|forecast for)\\s+([a-zA-Z\\s]+)\\b\", query, re.IGNORECASE)\n",
    "    # Default to the full query if no match\n",
    "    city = match.group(1).strip() if match else query.strip()\n",
    "    # Define the weather app URL for the API call\n",
    "    api_url = f\"{WEATHER_API_BASE_URL}/weather/?city_name={city}\"\n",
    "\n",
    "    # Make the API call to the weather-app\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        weather_data = response.json()\n",
    "\n",
    "        # Extract relevant weather information from the JSON response\n",
    "        temperature = weather_data.get(\"temperature\", \"N/A\")\n",
    "        cloud_cover = weather_data.get(\"cloud_cover\", \"N/A\")\n",
    "        humidity = weather_data.get(\"relative_humidity\", \"N/A\")\n",
    "        wind_speed = weather_data.get(\"wind_speed\", \"N/A\")\n",
    "\n",
    "        # Format the output message\n",
    "        weather_info = (f\"The current weather in {city} is as follows:\\n\"\n",
    "                        f\"Temperature: {temperature}Â°C\\n\"\n",
    "                        f\"Cloud Cover: {cloud_cover}%\\n\"\n",
    "                        f\"Humidity: {humidity}%\\n\"\n",
    "                        f\"Wind Speed: {wind_speed} km/h\")\n",
    "        return weather_info\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching weather data: {str(e)}\"\n",
    "\n",
    "# Analyze tool\n",
    "@tool\n",
    "def analyze(query: str):\n",
    "    \"\"\"Simulate an analysis tool for technical questions.\"\"\"\n",
    "    return f\"Analyzing the concept of {query}... The analysis is complete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9267b0-9526-4219-ba37-76ac09fb3d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weather App API URL\n",
    "WEATHER_API_BASE_URL = \"http://weather-app:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d905a0-c6f0-4ae8-9be3-0c0618a010c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize LLM\n",
    "Set up the Large Language Model (LLM) using the VLLM interface. This model will process inputs and optionally invoke tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "INFERENCE_SERVER_URL = os.getenv('API_URL_GRANITE')\n",
    "MODEL_NAME = \"granite-3-8b-instruct\"\n",
    "API_KEY= os.getenv('API_KEY_GRANITE')\n",
    "\n",
    "# LLM definition\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edf17e-4366-4f92-9461-64e25cf0ed55",
   "metadata": {},
   "source": [
    "Create a prompt template that the LLM will use to generate responses. This template guides the LLM to use tools when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e915928-a538-48d6-8547-7de9ccfbc9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "<|start_of_role|>system<|end_of_role|>\n",
    "You are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "If the question requires real-time information or specific technical analysis that you cannot provide directly, respond by indicating you will use a tool to retrieve the information and then use the tool to provide the accurate data.\n",
    "<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>user<|end_of_role|>\n",
    "Human: {input}\n",
    "<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>assistant<|end_of_role|>\n",
    "If this question requires real-time information or analysis, I will retrieve the data for you using a reliable source or tool.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63d21a-af73-40f6-908b-391eedd3b2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Conditional Logic for Workflow Continuation\n",
    "Create a function to determine whether the workflow should continue to the tools node or stop based on the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4344c515-9c7d-461e-9542-95e35344b3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List to track tool usage\n",
    "tools_used_log = []\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a242d-4ec6-4eb4-bb48-cd1faa389941",
   "metadata": {},
   "source": [
    "### Create the Function to Call the LLM\n",
    "Define a function that sends the formatted prompt to the LLM and decides whether to invoke any tools based on the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008fe1d3-ab4d-4852-8085-f580a1d1f4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    user_query = messages[-1].content.lower()\n",
    "\n",
    "    # Check if the query is about weather\n",
    "    if \"weather\" in user_query or \"forecast\" in user_query:\n",
    "        # Invoke the weather tool directly\n",
    "        tool_result = search.invoke(user_query)\n",
    "        if isinstance(tool_result, str):\n",
    "            ai_message = AIMessage(content=tool_result)\n",
    "        else:\n",
    "            ai_message = AIMessage(content=\"Error fetching weather data.\")\n",
    "        tools_used_log.append(\"search\")  # Log the tool usage\n",
    "    else:\n",
    "        # Process non-weather queries through the LLM\n",
    "        formatted_prompt = PROMPT.format(input=user_query)\n",
    "        try:\n",
    "            response = llm.invoke(formatted_prompt)\n",
    "            # Ensure response is a valid string\n",
    "            if not isinstance(response, str):\n",
    "                response = str(response)  # Convert to string if needed\n",
    "            ai_message = AIMessage(content=response)\n",
    "        except Exception as e:\n",
    "            # Handle unexpected errors from the LLM\n",
    "            ai_message = AIMessage(content=f\"Error processing your query: {str(e)}\")\n",
    "        # Handle analysis queries\n",
    "        if \"analyze\" in user_query or \"technical\" in user_query:\n",
    "            tool_result = analyze.invoke(user_query)\n",
    "            final_response = f\"{tool_result}. If you need further information, feel free to ask.\"\n",
    "            ai_message = AIMessage(content=final_response)\n",
    "            tools_used_log.append(\"analyze\")  # Log the tool usage\n",
    "\n",
    "    return {\"messages\": [ai_message]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22696a-59f5-400a-841a-29ee382219d4",
   "metadata": {},
   "source": [
    "### Agentic AI Workflow Logic\n",
    "First, we need to set the entry point for graph execution - agent node.\n",
    "\n",
    "Then we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state (MessageState). In our case, the destination is not known until the agent (LLM) decides.\n",
    "\n",
    "* Conditional edge: after the agent is called, we should either:\n",
    "  * Run tools if the agent said to take an action, OR\n",
    "  * Finish (respond to the user) if the agent did not ask to run tools\n",
    "* Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941837cf-2c21-4633-9c7e-82ee33eb6d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f6f7978a700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the workflow\n",
    "tools = [search, analyze]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# we initialize graph (StateGraph) by passing state schema (in our case MessagesState)\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# The agent node: responsible for deciding what (if any) actions to take.\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "\n",
    "# The tools node that invokes tools: if the agent decides to take an action, this node will then execute that action.\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point of the workflow to the \"agent\" node\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edges based on the LLM's response\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "\n",
    "# Add a normal edge from the \"tools\" node back to the \"agent\" node\n",
    "workflow.add_edge(\"tools\", 'agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab372594-a34c-4bc8-81e4-a9045542b061",
   "metadata": {},
   "source": [
    "### Compile the graph\n",
    "* When we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling .invoke(), .stream() and .batch() with your inputs\n",
    "* We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use MemorySaver - a simple in-memory checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2247b2e9-320e-4dad-99bc-fd2f5af64474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory\n",
    "# Use `MemorySaver` to allow the workflow to persist state between executions, maintaining context.\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be0c7aa-24d6-4fcf-abb9-6efbeab6df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the workflow into a runnable app\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5658820a-8158-46b9-b991-05cf61e42e69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(\"Graph visualization failed. Ensure you have the required dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc259e-fd41-4696-9716-eaa4a432b325",
   "metadata": {},
   "source": [
    "### Invoke the Workflow with an Example Input and Check the Tool Usage (if any)\n",
    "* Run the workflow with a sample input\n",
    "* Print which tools were used during the session, if any.\n",
    "\n",
    "\n",
    "1. LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \"agent\".\n",
    "2. The \"agent\" node executes, invoking the chat model.\n",
    "3. The chat model returns an AIMessage. LangGraph adds this to the state.\n",
    "4. Graph cycles the following steps until there are no more tool_calls on AIMessage:\n",
    "    * If AIMessage has tool_calls, \"tools\" node executes\n",
    "    * The \"agent\" node executes again and returns AIMessage\n",
    "5. Execution progresses to the special END value and outputs the final state. And as a result, we get a list of all our chat messages as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e88cc9b-5b44-44dc-a7f5-f02b39eee0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in dolomites is as follows:\n",
      "Temperature: -10.300000190734863Â°C\n",
      "Cloud Cover: 0.0%\n",
      "Humidity: 71.0%\n",
      "Wind Speed: 1.4843180179595947 km/h\n",
      "Tool used: search\n"
     ]
    }
   ],
   "source": [
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is the weather in Dolomites?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d88c99-ef82-446e-a33a-e035c7573426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but I don't have real-time data or personal opinions. However, I can provide some general information about both locations that might help you make a decision.\n",
      "\n",
      "The Dolomites and the Catalan Pyrenees are both popular hiking destinations, each with its own unique features.\n",
      "\n",
      "The Dolomites, located in Italy, are known for their dramatic peaks and valleys, which were formed by glacial erosion. They offer a wide range of hiking trails, from easy walks to challenging multi-day treks. The area is also famous for its stunning sunrises and sunsets, which can paint the peaks in warm, golden hues.\n",
      "\n",
      "The Catalan Pyrenees, on the other hand, are part of the larger Pyrenees mountain range that spans France, Spain, and Andorra. They offer a variety of landscapes, including dense forests, alpine meadows, and rugged peaks. The Catalan Pyrenees are known for their rich biodiversity and the opportunity to see wildlife such as ibex and eagles.\n",
      "\n",
      "In terms of which is \"better\" for hiking, it depends on your personal preferences. If you're looking for dramatic scenery and a wide range of trail difficulty, the Dolomites might be the better choice. If you're interested in seeing diverse wildlife and exploring a less crowded area, the Catalan Pyrenees could be more suitable.Analyzing the concept of analyze why dolomites is better than catalan pyrenees for hiking... The analysis is complete.. If you need further information, feel free to ask.\n",
      "Tool used: analyze\n"
     ]
    }
   ],
   "source": [
    "# Initialize tools_used_log before invoking the workflow\n",
    "tools_used_log = []  # Reset the log to ensure it's fresh for each session\n",
    "\n",
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"Analyze why Dolomites is better than Catalan Pyrenees for hiking\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed216840-9bf7-4165-9c57-3fd7f6cd4b14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To hike in the Dolomites, you should bring the following gear:\n",
      "\n",
      "1. Hiking boots with good grip and ankle support\n",
      "2. Layered clothing (moisture-wicking base layers, insulating mid-layers, and waterproof outer layers)\n",
      "3. A backpack to carry your gear\n",
      "4. A hat and gloves for warmth\n",
      "5. Sunglasses and sunscreen for protection from the sun\n",
      "6. A map and compass or GPS device\n",
      "7. A first-aid kit\n",
      "8. Food and water (at least 2 liters)\n",
      "9. A headlamp or flashlight (in case you're out later than expected)\n",
      "10. Trekking poles (optional, but helpful for steep ascents and descents)\n",
      "\n",
      "Always check the weather forecast before your hike and adjust your gear accordingly. It's also important to inform someone of your hiking plans and expected return time.content=\"To hike in the Dolomites, you should bring the following gear:\\n\\n1. Hiking boots with good grip and ankle support\\n2. Layered clothing (moisture-wicking base layers, insulating mid-layers, and waterproof outer layers)\\n3. A backpack to carry your gear\\n4. A hat and gloves for warmth\\n5. Sunglasses and sunscreen for protection from the sun\\n6. A map and compass or GPS device\\n7. A first-aid kit\\n8. Food and water (at least 2 liters)\\n9. A headlamp or flashlight (in case you're out later than expected)\\n10. Trekking poles (optional, but helpful for steep ascents and descents)\\n\\nAlways check the weather forecast before your hike and adjust your gear accordingly. It's also important to inform someone of your hiking plans and expected return time.\" response_metadata={'finish_reason': 'stop', 'model_name': 'granite30-8b'} id='run-9452d97d-40ea-4fb9-bab6-a0a4ac8ce0fd-0'\n",
      "No tools were used.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tools_used_log before invoking the workflow\n",
    "tools_used_log = []  # Reset the log to ensure it's fresh for each session\n",
    "\n",
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What gear should I bring to hike mountains like Dolomites?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2229369f-0d1f-40e8-b36b-f5741e523dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that are designed to act autonomously and make decisions on their own, rather than being controlled by humans. These systems are often used in situations where human intervention is not possible or practical, such as in self-driving cars or autonomous drones. The term \"agentic\" comes from the idea that these systems have a sense of agency, or the ability to act independently and make decisions based on their own internal processes.content='Agentic AI refers to artificial intelligence systems that are designed to act autonomously and make decisions on their own, rather than being controlled by humans. These systems are often used in situations where human intervention is not possible or practical, such as in self-driving cars or autonomous drones. The term \"agentic\" comes from the idea that these systems have a sense of agency, or the ability to act independently and make decisions based on their own internal processes.' response_metadata={'finish_reason': 'stop', 'model_name': 'granite30-8b'} id='run-59c97b26-828b-4745-bda5-e0e379738d5e-0'\n",
      "No tools were used.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tools_used_log before invoking the workflow\n",
    "tools_used_log = []  # Reset the log to ensure it's fresh for each session\n",
    "\n",
    "# Invoke the workflow with an example input\n",
    "input_prompt = \"What is Agentic AI?\"\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=input_prompt)]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "\n",
    "# Output the last response\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "# Inspect which tools were used during this session\n",
    "if not tools_used_log:\n",
    "    print(\"No tools were used.\")\n",
    "else:\n",
    "    for tool_name in tools_used_log:\n",
    "        print(f\"Tool used: {tool_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
