{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766aaa81-96e6-42dc-b29d-8216d2a7feec",
   "metadata": {},
   "source": [
    "## 2.1 Tools Calling\n",
    "\n",
    "In this notebook, we will explore how the LLM can use external tools to enhance its capabilities. We'll begin by adding and configuring a search tool to allow the LLM to perform real-time searches.\n",
    "\n",
    "By the end of this notebook, you'll be able to integrate and configure external search tools with a language model, and use them to perform real-time searches and enhance responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8007d-89cd-4f84-9eaf-88a7c4360296",
   "metadata": {},
   "source": [
    "#### Users, LLMs, Tools\n",
    "\n",
    "![LLM Tools](./llm_tools_llama.jpeg)\n",
    "\n",
    "1. **User Submits a Prompt**: The process starts when the user provides input or a prompt, which is sent to the **Executor** for handling.\n",
    "2. **Executor Forwards Prompt to LLM**: The **Executor** forwards the user’s prompt to the **LLM** (LLM) for initial processing.\n",
    "3. **LLM Processes the Prompt**: The **LLM** analyzes the prompt and determines whether it can generate a response directly or if external tools are needed to complete the request.\n",
    "4. **LLM Requests External Tool Invocation**: If external data or additional resources are required (e.g., real-time data from a search engine or a code interpreter), the **Model** signals the **Executor** to call the relevant tool.\n",
    "5. **Executor Calls the Tools**: The **Executor** initiates the tool request, calling external tools (such as Brave Search, Wolfram Alpha, Code Interpreter, or other custom tools) to fetch necessary information.\n",
    "6. **Tools Retrieve Data**: The external tools access the required information from outside environments (e.g., fetching real-time search results or performing complex calculations) and return the data to the **Executor**.\n",
    "7. **Executor Synthesizes Final Response**: The **Executor** combines the original prompt, the **LLM**'s processing, and any external tool responses. The final response is then sent back to the **LLM**.\n",
    "8. **LLM Generates Final Response**: The **LLM** synthesizes all the gathered data and generates a final response.\n",
    "9. **Executor Sends Response to User**: The **Executor** delivers the final response back to the user, completing the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f3fb-ee50-40f2-b276-b8194668e49e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210f6d4-0375-486e-ba37-8c25c5f18f10",
   "metadata": {},
   "source": [
    "#### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16ed2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q termcolor langchain_community duckduckgo_search wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bb3f0f-40b5-49a6-b493-5e361db0113e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cf3c2-f98d-4026-8232-72dab2ca7dcb",
   "metadata": {},
   "source": [
    "## 2. DuckDuckGo Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240fd72-f910-4ba4-82dc-c5ebf278452b",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7998b5-4373-4b70-bd81-9617f7ff6bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Tool Name: duckduckgo_search\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Verify tool name\n",
    "print(\"Search Tool Name:\", duckduckgo_search.name)\n",
    "\n",
    "# Adding the search tool to the list of available tools\n",
    "tools = [duckduckgo_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8179d-9841-45b0-b123-f62ae9fa5fa2",
   "metadata": {},
   "source": [
    "Render the tool’s description and ensures it is formatted correctly for integration into the LLM's prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee09847d-9ff7-4181-ad11-96186bf0a322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Name:\n",
      " duckduckgo_search\n",
      "Tools Description:\n",
      " duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n"
     ]
    }
   ],
   "source": [
    "# Render tool description to ensure it's ready for LLM usage\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_name = duckduckgo_search.name\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(\"Tools Name:\\n\", tools_name)\n",
    "print(\"Tools Description:\\n\", tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7c62-ea7d-4fd3-adcf-847beee5c0fb",
   "metadata": {},
   "source": [
    "## 3. Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bf848-656e-49ee-bc1e-7c4d2474678d",
   "metadata": {},
   "source": [
    "#### Define the Inference Model Server specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b908fd0-01dd-4ad2-b745-b3a4c56a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_URL = \"https://mistral-7b-instruct-v0-3-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443\"\n",
    "MODEL_NAME = \"mistral-7b-instruct\"\n",
    "API_KEY= os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b2f3f-ac23-4531-984b-6e8357233992",
   "metadata": {},
   "source": [
    "#### Create the LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01baa2b8-529d-455d-ad39-ef4a96dbaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM definition\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211bd1f-3e55-41ce-b3bc-0ee5db689c66",
   "metadata": {},
   "source": [
    "## 4. System Prompt for Tool Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b288a-dee4-4594-a4d3-ff2ad78445ba",
   "metadata": {},
   "source": [
    "Defines the system prompt template, which is used to instruct the LLM on how to interact\n",
    "with external tools. The prompt is structured to guide the LLM in calling tools when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd8bc4c-b353-4a51-a8b7-6cb348e19623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make sense or is not factually coherent, explain why instead of providing incorrect information. If you don't know the answer to a question, do not share false information.\n",
    "\n",
    "---\n",
    "\n",
    "You have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary. \n",
    "\n",
    "When using tools, follow this format:\n",
    "\n",
    "{{\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ \n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "You must always ensure the inputs are correct for the tool you call. Provide all output in JSON format.\n",
    "\n",
    "Be sure that is in JSON format the output, nothing else. Not repeate the json again.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "<<SYS>>\n",
    "Human: {input}\n",
    "AI:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\", \"tools_description\"], template=template)\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools)\n",
    "    .replace(\"{\", \"{{\")\n",
    "    .replace(\"}\", \"}}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4703b4-a1c4-4ee1-8712-38c955be1fb2",
   "metadata": {},
   "source": [
    "## 5. LLM Request\n",
    "\n",
    "Here, we issue a user query asking for the latest version of KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a87a8c-d861-43ad-b88e-7e0aef151000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"What is the latest version of KServe?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cf337-3d68-48f2-b059-104b1b088f94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create the Chain using the different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602a06f9-14d0-4a0b-87a7-baccd13befd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(tools_description)\n",
    "\n",
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93b325-9b93-4c04-a4c1-3186460e8476",
   "metadata": {
    "tags": []
   },
   "source": [
    "The LLM processes the request and responds\n",
    "with an action calling the DuckDuckGo search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bfcdf2-7271-4778-bc14-49cd7e553beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Latest version of KServe\",\n",
      "      \"description\": \"Search for the latest version of KServe\"\n",
      "    }\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=user_input, tools_description=tools_description);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1c9b9-9c3d-4b10-974b-8ce13ce2e7e8",
   "metadata": {},
   "source": [
    "## 6. Processing Tool Actions\n",
    "\n",
    "After the LLM provides its response, we will process the action request. If the LLM calls the DuckDuckGo search tool, we will execute the function and retrieve the results. The tool's output will then be formatted and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9d3ed0-e9b8-4b13-9320-efd909293fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LLM Response: {\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Latest version of KServe\",\n",
      "      \"description\": \"Search for the latest version of KServe\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Parse the LLM's response to extract the action and input\n",
    "print(f\"### LLM Response: {response}\")\n",
    "response_dict = json.loads(response)\n",
    "\n",
    "action = response_dict.get(\"action\")\n",
    "action_input = response_dict.get(\"action_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07949e0a-cf66-4d78-a778-1adb2cde4c9a",
   "metadata": {},
   "source": [
    "It then re-feeds the result back into the LLM for further processing (e.g., summarization or extraction of key information).\n",
    "\n",
    "Finally, we use the `process_tool_and_extract` function to dynamically perform a task (in this case, summarizing and extracting the key information from the search result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb494726-27f2-417a-a7fe-386a823a791d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Executing tool: duckduckgo_search\n",
      "----> Executing DuckDuckGo search for: Search for the latest version of KServe\n",
      "------> Search Result: KServe v0.13 enriches its Hugging Face runtime and now supports running Hugging Face models out-of-the-box. KServe v0.13 implements a KServe Hugging Face Serving Runtime, kserve-huggingfaceserver. With this implementation, KServe can now automatically infer a task from model architecture and select the optimized serving runtime. Currently ... KServe Python SDK is a library that enables Python models to be served by KServe, a Kubernetes-based model serving platform. It supports various storage providers, metrics, and handlers for pre/post processing, explainability, and liveness/readiness checks. KServe is an open-source project that enables serverless inferencing on Kubernetes for various machine learning frameworks. Learn how to use KServe with Kubeflow, the platform for production machine learning. KServe is an open source project that provides a standard inference platform for serving machine learning models on Kubernetes. It supports various frameworks, formats, and protocols, and offers features like auto-scaling, canary deployments, and explainability. KServe is a highly scalable and standards-based model inference platform on Kubernetes for scalable AI. Open Data Hub (ODH) now seamlessly incorporates KServe as an integral component within its ecosystem. In earlier iterations, utilizing KServe necessitated the execution of a relatively intricate installation script. However, with the release of Open Data Hub 2.5, significant strides have ...\n",
      "\n",
      "\n",
      "##### Answer to the User Input Question: 'What is the latest version of KServe?' #####\n",
      "The latest version of KServe is v0.13, which enriches its Hugging Face runtime and supports running Hugging Face models out-of-the-box."
     ]
    }
   ],
   "source": [
    "def process_tool_and_extract(action, action_input, task):\n",
    "    for tool in tools:\n",
    "        if tool.name == action:\n",
    "            print(f\"--> Executing tool: {tool.name}\")\n",
    "            try:\n",
    "                query = action_input['query']['description']\n",
    "                print(f\"----> Executing DuckDuckGo search for: {query}\")\n",
    "\n",
    "                result = tool.invoke(query)\n",
    "                print(f\"------> Search Result: {result}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # Dynamically create a prompt based on the task (summarize or extract information)\n",
    "                task_prompt = f\"Based on the following search result, please {task},{user_input}:\\n\\n{result}\"\n",
    "\n",
    "                # Feed the search result back into the LLM for summarization or extraction\n",
    "                print(f\"##### Answer to the User Input Question: '{user_input}' #####\")\n",
    "                summary_response = conversation.predict(input=task_prompt, tools_description=tools_description)\n",
    "                #print(f\"Task Result: {summary_response}\")\n",
    "                \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing tool {action}: {str(e)}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Tool {action} not found or unsupported operation.\")\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "task = \"Summarize and extract the key information of the text. Give me a sentence with that as an answer, not in json, just plain text\"\n",
    "process_tool_and_extract(action, action_input, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ce659-f532-4216-80e7-a7fb099e09ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bdae5ff-8e42-4ede-8ac4-5ff988c1cf80",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this notebook, we've successfully extended the capabilities of the LLM by integrating an external tool (DuckDuckGo search). We covered:\n",
    "\n",
    "- **LLM and Tool Integration:** How to modify the system prompt for tool access.\n",
    "- **Tool Setup and Configuration:** Setting up the DuckDuckGo search tool for real-time information retrieval.\n",
    "- **Making Requests and Handling Responses:** Using the tool during LLM interactions and processing the results.\n",
    "\n",
    "With these tools in place, you're now equipped to extend the LLM's capabilities even further by adding more tools and refining its behavior. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
