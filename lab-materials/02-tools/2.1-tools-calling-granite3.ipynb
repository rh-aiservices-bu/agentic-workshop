{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766aaa81-96e6-42dc-b29d-8216d2a7feec",
   "metadata": {},
   "source": [
    "## 2.1 Tools Calling\n",
    "\n",
    "In this notebook, we will explore how the LLM can use external tools to enhance its capabilities. We'll begin by adding and configuring a search tool to allow the LLM to perform real-time searches.\n",
    "\n",
    "By the end of this notebook, you'll be able to integrate and configure external search tools with a language model, and use them to perform real-time searches and enhance responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a662971-eece-42f0-93c9-20ff47d822bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "![LLM Tools](../../content/modules/ROOT/assets/images/04/04-02-tools-diagram.png)\n",
    "\n",
    "Through tool calling, LLMs can:\n",
    "\n",
    "- **Access Real-Time Data**: Retrieve current information from search engines, databases, or other real-time sources.\n",
    "- **Perform Specialized Calculations**: Use tools like code interpreters to execute complex calculations or data transformations.\n",
    "- **Format Responses for Structured Systems**: Generate responses in specific schemas required by APIs or databases, ensuring smooth communication with these systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f3fb-ee50-40f2-b276-b8194668e49e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210f6d4-0375-486e-ba37-8c25c5f18f10",
   "metadata": {},
   "source": [
    "#### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ed2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain openai\n",
    "!pip install langchain-openai\n",
    "!pip install \"langchain-core==0.3.27\"\n",
    "!pip install langchain_community\n",
    "!pip install -q termcolor langchain_community duckduckgo_search wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb3f0f-40b5-49a6-b493-5e361db0113e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cf3c2-f98d-4026-8232-72dab2ca7dcb",
   "metadata": {},
   "source": [
    "## 2. DuckDuckGo Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240fd72-f910-4ba4-82dc-c5ebf278452b",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will configure the tool by initializing the DuckDuckGo search functionality. This will be the primary tool for real-time search queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7998b5-4373-4b70-bd81-9617f7ff6bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Tool Name: duckduckgo_search\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize DuckDuckGo Search Tool\n",
    "duckduckgo_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Verify tool name\n",
    "print(\"Search Tool Name:\", duckduckgo_search.name)\n",
    "\n",
    "# Adding the search tool to the list of available tools\n",
    "tools = [duckduckgo_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8179d-9841-45b0-b123-f62ae9fa5fa2",
   "metadata": {},
   "source": [
    "Render the tool’s description and ensures it is formatted correctly for integration into the LLM's prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09847d-9ff7-4181-ad11-96186bf0a322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Name:\n",
      " duckduckgo_search\n",
      "Tools Description:\n",
      " duckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{'query': {{'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}}}\n"
     ]
    }
   ],
   "source": [
    "# Render tool description to ensure it's ready for LLM usage\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "\n",
    "tools_name = duckduckgo_search.name\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(\"Tools Name:\\n\", tools_name)\n",
    "print(\"Tools Description:\\n\", tools_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b7c62-ea7d-4fd3-adcf-847beee5c0fb",
   "metadata": {},
   "source": [
    "## 3. Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94bf848-656e-49ee-bc1e-7c4d2474678d",
   "metadata": {},
   "source": [
    "#### Define the Inference Model Server specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b908fd0-01dd-4ad2-b745-b3a4c56a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_SERVER_URL = os.getenv('API_URL_GRANITE')\n",
    "MODEL_NAME = \"granite-3-8b-instruct\"\n",
    "API_KEY= os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b2f3f-ac23-4531-984b-6e8357233992",
   "metadata": {},
   "source": [
    "#### Create the LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa2b8-529d-455d-ad39-ef4a96dbaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM definition\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211bd1f-3e55-41ce-b3bc-0ee5db689c66",
   "metadata": {},
   "source": [
    "## 4. System Prompt for Tool Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b288a-dee4-4594-a4d3-ff2ad78445ba",
   "metadata": {},
   "source": [
    "Defines the system prompt template, which is used to instruct the LLM on how to interact\n",
    "with external tools. The prompt is structured to guide the LLM in calling tools when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8bc4c-b353-4a51-a8b7-6cb348e19623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "<|start_of_role|>system<|end_of_role|>\n",
    "You are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make sense or is not factually coherent, explain why instead of providing incorrect information. If you don't know the answer to a question, do not share false information.\n",
    "\n",
    "---\n",
    "\n",
    "You have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary. \n",
    "\n",
    "When using tools, follow this format:\n",
    "\n",
    "{{\n",
    "  \"action\": \"Specify the tool you want to use.\",\n",
    "  \"action_input\": {{ \n",
    "    \"key\": \"Value inputs to the tool in valid JSON format.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "You must always ensure the inputs are correct for the tool you call. Provide all output in JSON format.\n",
    "\n",
    "Be sure that is in JSON format the output, nothing else. Not repeate the json again.\n",
    "\n",
    "The available tools include:\n",
    "\n",
    "{tools_description}\n",
    "\n",
    "<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>user<|end_of_role|>\n",
    "Human: {input}\n",
    "<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>assistant<|end_of_role|>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "tools_description = (\n",
    "    render_text_description_and_args(tools)\n",
    "    .replace(\"{\", \"{{\")\n",
    "    .replace(\"}\", \"}}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4703b4-a1c4-4ee1-8712-38c955be1fb2",
   "metadata": {},
   "source": [
    "## 5. LLM Request\n",
    "\n",
    "Here, we issue a user query asking for the latest version of KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a87a8c-d861-43ad-b88e-7e0aef151000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"What are the last 5 year results of the S&P500?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cf337-3d68-48f2-b059-104b1b088f94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create the Chain using the different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602a06f9-14d0-4a0b-87a7-baccd13befd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_920/3274098412.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  conversation = LLMChain(llm=llm,\n"
     ]
    }
   ],
   "source": [
    "#print(tools_description)\n",
    "\n",
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93b325-9b93-4c04-a4c1-3186460e8476",
   "metadata": {
    "tags": []
   },
   "source": [
    "The LLM processes the request and responds\n",
    "with an action calling the DuckDuckGo search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bfcdf2-7271-4778-bc14-49cd7e553beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Last 5 years S&P 500 results\",\n",
      "      \"description\": \"S&P 500 yearly performance from the past 5 years\"\n",
      "    }\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "response = conversation.predict(input=user_input, tools_description=tools_description);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1c9b9-9c3d-4b10-974b-8ce13ce2e7e8",
   "metadata": {},
   "source": [
    "## 6. Processing Tool Actions\n",
    "\n",
    "After the LLM provides its response, we will process the action request. If the LLM calls the DuckDuckGo search tool, we will execute the function and retrieve the results. The tool's output will then be formatted and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9d3ed0-e9b8-4b13-9320-efd909293fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LLM Response: {\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": {\n",
      "    \"query\": {\n",
      "      \"title\": \"Last 5 years S&P 500 results\",\n",
      "      \"description\": \"S&P 500 yearly performance from the past 5 years\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Parse the LLM's response to extract the action and input\n",
    "print(f\"### LLM Response: {response}\")\n",
    "response_dict = json.loads(response)\n",
    "\n",
    "action = response_dict.get(\"action\")\n",
    "action_input = response_dict.get(\"action_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07949e0a-cf66-4d78-a778-1adb2cde4c9a",
   "metadata": {},
   "source": [
    "It then re-feeds the result back into the LLM for further processing (e.g., summarization or extraction of key information).\n",
    "\n",
    "Finally, we use the `process_tool_and_extract` function to dynamically perform a task (in this case, summarizing and extracting the key information from the search result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb494726-27f2-417a-a7fe-386a823a791d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Executing tool: duckduckgo_search\n",
      "----> Executing DuckDuckGo search for: S&P 500 yearly performance from the past 5 years\n",
      "------> Search Result: S&P 500 5 Year Return is at 87.83%, compared to 93.58% last month and 54.65% last year. This is higher than the long term average of 46.17%. The S&P 500 5 Year Return is the investment return received for a 5 year period, excluding dividends, when holding the S&P 500 index. 5 year chart of the S&P 500 stock index* The 5 year chart of S&P 500 summarizes the chages in the price well, however, we recommend to have a look at the chart(s) below, too. Similar charts of the past 10 year can be found here. You can find charts of other periods (1, 3, 6, 12 months etc.) here. 5 years return and graph of S&P 500* When looking at the Periods in the Price Performance table, the 5-Day through 2-Year periods are based on daily data, the 3-Year and 5-Year periods are based on weekly data, and the 10-Year and 20-Year periods are based on monthly data. Barchart Symbol Notes Tutorial (8:43) New Highs The S&P 500 has gained about 10.5% annually since its introduction in 1957. The S&P 500's annual average return in 2023 was 26.3%, a significant increase from the -18.1% return in 2022. The S&P 500 is a market-capitalization-weighted index of the 500 leading publicly traded companies in the U.S. The index acts as a benchmark of the performance of the U.S. stock market overall ...\n",
      "\n",
      "\n",
      "##### Answer to the User Input Question: 'What are the last 5 year results of the S&P500?' #####\n",
      "The S&P 500 5-year return as of the latest data is 87.83%, which is higher than the long-term average of 46.17%. This return is an increase from the 54.65% recorded last year and a significant increase from the -18.1% return in 2022."
     ]
    }
   ],
   "source": [
    "def process_tool_and_extract(action, action_input, task):\n",
    "    for tool in tools:\n",
    "        if tool.name == action:\n",
    "            print(f\"--> Executing tool: {tool.name}\")\n",
    "            try:\n",
    "                query = action_input['query']\n",
    "                print(f\"----> Executing DuckDuckGo search for: {query}\")\n",
    "\n",
    "                result = tool.invoke(query)\n",
    "                print(f\"------> Search Result: {result}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # Dynamically create a prompt based on the task (summarize or extract information)\n",
    "                task_prompt = f\"Based on the following search result, please {task},{user_input}:\\n\\n{result}\"\n",
    "\n",
    "                # Feed the search result back into the LLM for summarization or extraction\n",
    "                print(f\"##### Answer to the User Input Question: '{user_input}' #####\")\n",
    "                summary_response = conversation.predict(input=task_prompt, tools_description=tools_description)\n",
    "                #print(f\"Task Result: {summary_response}\")\n",
    "                \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing tool {action}: {str(e)}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Tool {action} not found or unsupported operation.\")\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "task = \"Summarize and extract the key information of the text. Give me a sentence with that as an answer, not in json, just plain text.\"\n",
    "process_tool_and_extract(action, action_input, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ce659-f532-4216-80e7-a7fb099e09ed",
   "metadata": {},
   "source": [
    "## LLMs Tools Calling Architecture\n",
    "\n",
    "![LLM Tools](./llm_tools_llama.jpeg)\n",
    "\n",
    "1. **User Submits a Prompt**: The process starts when the user provides input or a prompt, which is sent to the **Executor** for handling.\n",
    "2. **Executor Forwards Prompt to LLM**: The **Executor** forwards the user’s prompt to the **LLM** (LLM) for initial processing.\n",
    "3. **LLM Processes the Prompt**: The **LLM** analyzes the prompt and determines whether it can generate a response directly or if external tools are needed to complete the request.\n",
    "4. **LLM Requests External Tool Invocation**: If external data or additional resources are required (e.g., real-time data from a search engine or a code interpreter), the **Model** signals the **Executor** to call the relevant tool.\n",
    "5. **Executor Calls the Tools**: The **Executor** initiates the tool request, calling external tools (such as Brave Search, Wolfram Alpha, Code Interpreter, or other custom tools) to fetch necessary information.\n",
    "6. **Tools Retrieve Data**: The external tools access the required information from outside environments (e.g., fetching real-time search results or performing complex calculations) and return the data to the **Executor**.\n",
    "7. **Executor Synthesizes Final Response**: The **Executor** combines the original prompt, the **LLM**'s processing, and any external tool responses. The final response is then sent back to the **LLM**.\n",
    "8. **LLM Generates Final Response**: The **LLM** synthesizes all the gathered data and generates a final response.\n",
    "9. **Executor Sends Response to User**: The **Executor** delivers the final response back to the user, completing the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdae5ff-8e42-4ede-8ac4-5ff988c1cf80",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this notebook, we've successfully extended the capabilities of the LLM by integrating an external tool (DuckDuckGo search). We covered:\n",
    "\n",
    "- **LLM and Tool Integration:** How to modify the system prompt for tool access.\n",
    "- **Tool Setup and Configuration:** Setting up the DuckDuckGo search tool for real-time information retrieval.\n",
    "- **Making Requests and Handling Responses:** Using the tool during LLM interactions and processing the results.\n",
    "\n",
    "With these tools in place, you're now equipped to extend the LLM's capabilities even further by adding more tools and refining its behavior. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
